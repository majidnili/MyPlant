{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib2\n",
    "from string import ascii_uppercase\n",
    "from bs4 import NavigableString\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import string\n",
    "from scipy.sparse import hstack, vstack\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import savefig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#scraping data:\n",
    "html_str = \"http://www.missouribotanicalgarden.org/PlantFinder/PlantFinderListResults.aspx?letter=\"\n",
    "missouri = \"http://www.missouribotanicalgarden.org\"\n",
    "def get_html(missouri,html_str):\n",
    "    plants = set()\n",
    "    for letter in ascii_uppercase:\n",
    "        html = html_str + letter\n",
    "        r = requests.get(html)\n",
    "        soup = BeautifulSoup(r.content,'html.parser')\n",
    "        links = soup.find_all('a',target = '_self')\n",
    "        for link in links:\n",
    "            if link['href'][-1] == letter:\n",
    "                plants.add(missouri +link['href'])\n",
    "    plants = sorted(list(plants))\n",
    "    return plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_soup(plants):\n",
    "    plant_dict = dict()\n",
    "    for plant in plants:\n",
    "        r = requests.get(plant)\n",
    "        soup = BeautifulSoup(r.content,'html.parser')\n",
    "        name = soup.find('span',id=\"dnn_srTitle_lblTitle\").text\n",
    "        plant_dict[name] = soup\n",
    "    return plant_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function extract inforamtion from web pages made by make_soup function\n",
    "# and make a dictionary for each plant.\n",
    "missouri = \"http://www.missouribotanicalgarden.org\"\n",
    "def make_temp_dict(plant_dict):\n",
    "    temp_dict =dict()\n",
    "    for key,value in plant_dict.iteritems():\n",
    "        key = key.lower()\n",
    "        try:\n",
    "            key = key.strip()\n",
    "        except:\n",
    "            pass\n",
    "    # extracting data from right column (table):\n",
    "        table = dict()\n",
    "        for x in value.select('.column-right > div'):\n",
    "            for y in x.contents:\n",
    "                if type(y) == NavigableString:\n",
    "                    try:\n",
    "                        txt = y.strip()\n",
    "                    except:\n",
    "                        txt = y\n",
    "                    categ = txt.split(':')\n",
    "                    try:\n",
    "                        table[categ[0]] = categ[1]\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "    # extracting text part:\n",
    "        text = list()\n",
    "        for x in value.select('.row > p'):\n",
    "            for y in x.contents:\n",
    "                if type(y) == NavigableString:\n",
    "                    try:\n",
    "                        text.append(y.strip() + ' ')\n",
    "                    except:\n",
    "                        text.append(y)\n",
    "                else:\n",
    "                    pass\n",
    "        table['body'] = ''.join(text)\n",
    "        temp_dict[key] = table.copy()\n",
    "        \n",
    "    # extracting url and image-url (if available):\n",
    "        link = value.select('form')\n",
    "        \n",
    "        temp_dict[key]['url'] = missouri + link[0][\"action\"]\n",
    "                        \n",
    "        try:\n",
    "            link = value.select('.main-pic > a')\n",
    "            imgUrl = missouri + link[0]['href']\n",
    "            r = requests.get(imgUrl)\n",
    "            soup2 = BeautifulSoup(r.content,'html.parser') \n",
    "            link2 = soup2.select('img')\n",
    "            temp_dict[key]['img-url'] = link2[0]['src']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert some categorical features of plant profile (right column table) to numerical values:\n",
    "def category_to_num(temp_dict):\n",
    "# check if the plant blooms (ignore details of blooming/flower)\n",
    "    set_flower = set()\n",
    "    bloom_dict=dict()\n",
    "# list various terms for blooming plants:\n",
    "    for key in temp_dict:\n",
    "        try:\n",
    "            set_flower.add(temp_dict[key][\"Flower\"])\n",
    "        except:\n",
    "            pass\n",
    "    for i in set_flower:\n",
    "        if  i == ' Insignificant' or i == ' Fragrant, Insignificant' or i == ' Insignificant, Good Dried' \\\n",
    "            or i == ' Showy, Insignificant' :\n",
    "            bloom_dict[i] = -1.\n",
    "        else:\n",
    "            bloom_dict[i] = 1.\n",
    "\n",
    "# Maintenance level:\n",
    "    temp_list = [' Low',' Low-Medium',' Medium',' Medium-High',' High']\n",
    "    maintn_dict = dict()\n",
    "    for i,k in enumerate(temp_list):\n",
    "         maintn_dict[k] = float(i+1)\n",
    "\n",
    "# light requirement:\n",
    "    temp_list = [' Full Shade',' Part shade to full shade',' Part shade',' Full sun to part shade',' Full sun']\n",
    "    light_dict = dict()\n",
    "    for i,k in enumerate(temp_list):\n",
    "         light_dict[k] = float(i+1)\n",
    "\n",
    "# watering condition:            \n",
    "    temp_list = [' Dry', ' Dry to medium', ' Medium', ' Medium to wet', ' Wet']\n",
    "    water_dict = dict()\n",
    "    for i,k in enumerate(temp_list):\n",
    "         water_dict[k] = float(i+1)\n",
    "    \n",
    "    dict_list = bloom_dict,maintn_dict,light_dict,water_dict\n",
    "    return dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a matrix of some of the features of the table part of plant profile built by category_to_num function:\n",
    "def make_table(dict_list,temp_dict,keys):\n",
    "    plant_table = defaultdict(list)\n",
    "    name_list = [\"Flower\",\"Maintenance\",\"Sun\",\"Water\"]\n",
    "    name_list2 = [\"Height\",\"Spread\"]\n",
    "    \n",
    "    for key in keys:\n",
    "        for name,feature in zip(name_list,dict_list):\n",
    "            try:\n",
    "                level = temp_dict[key][name]\n",
    "                plant_table[key].append(feature[level])\n",
    "            except:\n",
    "                plant_table[key].append(-1.)\n",
    "                \n",
    "    # try new feature: sun_level/water_level\n",
    "    # higher number is for desert plants and lower is for jungle ground plants(?)\n",
    "        if plant_table[key][2] != -1. and plant_table[key][3] != -1.:\n",
    "            plant_table[key].append(plant_table[key][2]/plant_table[key][3])\n",
    "        else:\n",
    "            plant_table[key].append(-1.)\n",
    "\n",
    "    # make columns from size of the plants:\n",
    "        for feature in name_list2:\n",
    "            try:\n",
    "                range_ = temp_dict[key][feature]\n",
    "                low = float(range_.split()[0])\n",
    "                high = float(range_.split()[2])\n",
    "                plant_table[key].append(low)\n",
    "                plant_table[key].append(high)\n",
    "            except:\n",
    "                plant_table[key].append(-1.)\n",
    "                plant_table[key].append(-1.)\n",
    "                \n",
    "        try:\n",
    "            range_ = temp_dict[key]['Zone']\n",
    "            low = float(range_.split()[0])\n",
    "            high = float(range_.split()[-1])\n",
    "            plant_table[key].append(low)\n",
    "            plant_table[key].append(high)\n",
    "        except:\n",
    "            plant_table[key].append(-1.)\n",
    "            plant_table[key].append(-1.)\n",
    "\n",
    "        # check if plant is houseplant            \n",
    "        words = ['houseplant','indoor']\n",
    "        houseplant = dict()\n",
    "        exclude = set(string.punctuation)\n",
    "        houseplant[key] = 0.\n",
    "        for word in temp_dict[key][\"body\"].split():\n",
    "            word_ = ''.join(ch for ch in list(word) if ch not in exclude)\n",
    "            if word_ in words:\n",
    "                houseplant[key] = 1.\n",
    "                break\n",
    "\n",
    "        plant_table[key].append(houseplant[key])\n",
    "    return plant_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordnet = WordNetLemmatizer()\n",
    "def tokenize(doc):\n",
    "    return [wordnet.lemmatize(word) for word in word_tokenize(doc.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenizer_(descriptions,tokenize,ngram = (1,4),maxf = None):\n",
    "    tfidf = TfidfVectorizer(stop_words='english',ngram_range=ngram,strip_accents='unicode',max_features=maxf,tokenizer=tokenize)\n",
    "    vectorized = tfidf.fit_transform(descriptions)\n",
    "    return vectorized,tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cosine_dist(plant,vectorized,njobz=1):\n",
    "    sims = pairwise_distances(vectorized[plant,:],vectorized,metric='cosine',n_jobs=njobz)\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys = [key for key in temp_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_list = category_to_num(temp_dict)\n",
    "plant_table = make_table(dict_list,temp_dict,keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make matrix from table part from make_table functions:\n",
    "table = np.zeros(len(plant_table) * len(plant_table[keys[0]])).reshape(len(plant_table),len(plant_table[keys[0]]))\n",
    "for i,key in enumerate(keys):\n",
    "    table[i,:] = plant_table[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find similarity by concatinating the matrix and vectorized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compile a descriptions list from plant profile for nlp: \n",
    "descriptions = []\n",
    "for key in keys:\n",
    "    text = [v for k,v in temp_dict[key].iteritems() if k != \"url\" and k != \"img-url\"]\n",
    "    descriptions.append(''.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorized_all, tfidf_all = tokenizer_(descriptions,tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform dimensionality reduction on tokenized matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=1000,algorithm=\"arpack\")\n",
    "svd.fit(vectorized_all)\n",
    "reduced = svd.fit_transform(vectorized_all)\n",
    "eig = svd.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot percentage of variance explained by each of the selected components to decide how many components to keep\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(eig,linewidth=5)\n",
    "plt.legend(prop={'size':10})\n",
    "plt.ylabel(\"magnitude\",fontsize=20)\n",
    "plt.xlabel(\"feature\",fontsize=20)\n",
    "plt.tick_params(axis='both',labelsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_redu = reduced[:,:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#normalize the table matrix\n",
    "nrm = np.linalg.norm(table,axis=1)\n",
    "norm_table = table.copy()\n",
    "for i in xrange(table.shape[0]):\n",
    "    norm_table[i,:] = table[i,:]/nrm[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalize the svd matrix\n",
    "nrm = np.linalg.norm(tfidf_redu,axis=1)\n",
    "norm_tfidf_redu = tfidf_redu.copy()\n",
    "for i in xrange(tfidf_redu.shape[0]):\n",
    "    norm_tfidf_redu[i,:] = tfidf_redu[i,:]/nrm[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# concatinate the normalized svd and table matrices:\n",
    "final_mat = np.concatenate((norm_table,norm_tfidf_redu),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate pairwise distances for selected plants\n",
    "plant = \"anthurium andraeanum\"\n",
    "sim = cosine_dist(keys.index(plant),final_mat)\n",
    "similar = sim_all[0].argsort()[:40]\n",
    "sim_plants = [(j,keys[j],sim_all[0][j]) for j in similar]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
